{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryajani/algo-trading/blob/main/algotrade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zNqK39YROyB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLHcWzYsNhaR"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import gspread\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fetch_data(ticker, start_date, end_date):\n",
        "    # Define the ticker symbol for Nifty 50\n",
        "    nifty_ticker = ticker\n",
        "\n",
        "    # Fetch data for Nifty 50\n",
        "    nifty_data = yf.Ticker(nifty_ticker)\n",
        "\n",
        "    # Get historical market data (CHANGE TO 10 YEARS)\n",
        "    historical_data = nifty_data.history(start=start_date, end=end_date)  # Fetch data for the past month\n",
        "\n",
        "    # Drop unwanted columns: 'Volume', 'Dividends', 'Stock Splits'\n",
        "    cleaned_data = historical_data.drop(columns=[\"Volume\", \"Dividends\", \"Stock Splits\"])\n",
        "\n",
        "    cleaned_data.index = cleaned_data.index.astype(str)\n",
        "    cleaned_data = cleaned_data.reset_index()\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "\n",
        "def find_bestMA(data):\n",
        "\n",
        "    max_profV = 0\n",
        "\n",
        "    for sma1 in range(10, 30):\n",
        "        for sma2 in range(50, 100):\n",
        "\n",
        "            signals = []\n",
        "            entry_price = None\n",
        "            exit_price = None\n",
        "            prev_pos = None\n",
        "            profP = []\n",
        "\n",
        "            data[f'{sma1}SMA'] = data['Close'].rolling(window=sma1).mean()\n",
        "            data[f'{sma2}SMA'] = data['Close'].rolling(window=sma2).mean()\n",
        "\n",
        "            for i in range(len(data)):\n",
        "                if i == 0:  # First row, no crossover to check\n",
        "                    signals.append(\"Hold\")\n",
        "                    profP.append(None)\n",
        "                else:\n",
        "                    # Check for crossovers\n",
        "                    prev_sma1 = data[f'{sma1}SMA'].iloc[i - 1]\n",
        "                    prev_sma2 = data[f'{sma2}SMA'].iloc[i - 1]\n",
        "                    curr_sma1 = data[f'{sma1}SMA'].iloc[i]\n",
        "                    curr_sma2 = data[f'{sma2}SMA'].iloc[i]\n",
        "\n",
        "                    if prev_sma1 <= prev_sma2 and curr_sma1 > curr_sma2:\n",
        "                        signals.append(\"Long\")  # Bullish crossover\n",
        "                        curr_signal = \"Long\"\n",
        "                    elif prev_sma1 >= prev_sma2 and curr_sma1 < curr_sma2:\n",
        "                        signals.append(\"Short\")  # Bearish crossover\n",
        "                        curr_signal = \"Short\"\n",
        "                    else:\n",
        "                        signals.append(\"Hold\")  # No crossover\n",
        "                        curr_signal = \"Hold\"\n",
        "\n",
        "                    exit_price = data[\"Close\"].iloc[i]\n",
        "\n",
        "                    if curr_signal == \"Long\" or curr_signal == \"Short\":\n",
        "                        prev_pos = curr_signal\n",
        "                        if not entry_price:\n",
        "                            entry_price = exit_price\n",
        "                            profP.append(None)\n",
        "                        else:\n",
        "                            curr_profP = (entry_price-exit_price)*100/entry_price\n",
        "                            if curr_signal == \"Short\": # exit long\n",
        "                                curr_profP *= -1\n",
        "                            entry_price = exit_price\n",
        "                            profP.append(curr_profP)\n",
        "                    elif i == len(data)-1:\n",
        "                        curr_profP = (entry_price-exit_price)*100/entry_price\n",
        "                        if prev_pos == \"Short\":\n",
        "                            curr_profP *= -1\n",
        "                        profP.append(curr_profP)\n",
        "                    else:\n",
        "                        profP.append(None)\n",
        "\n",
        "            curr_sma1_val = data[f'{sma1}SMA']\n",
        "            curr_sma2_val = data[f'{sma2}SMA']\n",
        "            data['ProfP'] = profP\n",
        "            sum_profP = data['ProfP'].sum()\n",
        "\n",
        "            data = data.drop(columns=[f'{sma1}SMA', f'{sma2}SMA', 'ProfP'])\n",
        "\n",
        "            if sum_profP > max_profV:\n",
        "                max_profV = sum_profP\n",
        "                best_signals = signals\n",
        "                best_sma1 = sma1\n",
        "                best_sma2 = sma2\n",
        "                best_sma1_val = curr_sma1_val\n",
        "                best_sma2_val = curr_sma2_val\n",
        "                best_profP = profP\n",
        "\n",
        "\n",
        "    data['Signal'] = best_signals\n",
        "    data[f'{best_sma1}SMA'] = best_sma1_val\n",
        "    data[f'{best_sma2}SMA'] = best_sma2_val\n",
        "    data['Profit Percentage'] = best_profP\n",
        "    data['Profit Value'] = margin/data['Close'] * data['Profit Percentage']\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "margin = 100000\n",
        "cleaned_data = fetch_data(\"^NSEI\", \"2014-12-30\", \"2024-12-30\")\n",
        "\n",
        "# Apply the function to detect signals\n",
        "cleaned_data = find_bestMA(cleaned_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "iter = []\n",
        "iterV = []\n",
        "\n",
        "\n",
        "# Ensure 'Date' column is in datetime format\n",
        "cleaned_data['Date'] = pd.to_datetime(cleaned_data['Date'])\n",
        "\n",
        "# Find the minimum and maximum dates\n",
        "min_date = cleaned_data['Date'].min()\n",
        "max_date = cleaned_data['Date'].max()\n",
        "\n",
        "# Calculate the total number of days between the min and max date\n",
        "number_days = (max_date - min_date).days\n",
        "\n",
        "# Extract year and month (year-month format)\n",
        "cleaned_data['Year_Month'] = cleaned_data['Date'].dt.to_period('M')\n",
        "\n",
        "# Find the number of unique year-month combinations\n",
        "number_months = cleaned_data['Year_Month'].nunique()\n",
        "\n",
        "cleaned_data = cleaned_data.drop(columns=['Year_Month'])\n",
        "cleaned_data['Date'] = cleaned_data['Date'].astype(str)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" METRICS \"\"\"\n",
        "\n",
        "# total profit percentage\n",
        "profit_sum_p = cleaned_data['Profit Percentage'].sum()\n",
        "iter.append('Profit Percentage')\n",
        "iterV.append(profit_sum_p)\n",
        "\n",
        "# total profit value\n",
        "profit_sum_v = cleaned_data['Profit Value'].sum()\n",
        "iter.append('Profit Value')\n",
        "iterV.append(profit_sum_v)\n",
        "\n",
        "# max profit percentage\n",
        "profit_max_p = cleaned_data['Profit Percentage'].max()\n",
        "iter.append('Max Profit Percentage')\n",
        "iterV.append(profit_max_p)\n",
        "\n",
        "# max loss percentage\n",
        "profit_min_p = cleaned_data['Profit Percentage'].min()\n",
        "iter.append('Min Profit Percentage')\n",
        "iterV.append(profit_min_p)\n",
        "\n",
        "# number of win days\n",
        "num_win_days = (cleaned_data['Profit Percentage'] > 0).sum()\n",
        "iter.append('Number of Win Days')\n",
        "iterV.append(num_win_days)\n",
        "\n",
        "# number of loss days\n",
        "num_loss_days = (cleaned_data['Profit Percentage'] <= 0).sum()\n",
        "iter.append('Number of Loss Days')\n",
        "iterV.append(num_loss_days)\n",
        "\n",
        "# average monthly profit value\n",
        "avg_month_profit = profit_sum_v/number_months\n",
        "iter.append('Average monthly profit value')\n",
        "iterV.append(avg_month_profit)\n",
        "\n",
        "# average monthly profit percent\n",
        "avg_month_profit = profit_sum_p/number_months\n",
        "iter.append('Average monthly profit percent')\n",
        "iterV.append(avg_month_profit)\n",
        "\n",
        "# average profit on win days\n",
        "sum_win_profit_v = cleaned_data[cleaned_data['Profit Value'] >= 0]['Profit Value'].sum()\n",
        "avg_win_profit_v = sum_win_profit_v/num_win_days\n",
        "iter.append('Average win day profit value')\n",
        "iterV.append(avg_win_profit_v)\n",
        "\n",
        "# average profit on loss days\n",
        "sum_win_profit_v = cleaned_data[cleaned_data['Profit Value'] < 0]['Profit Value'].sum()\n",
        "avg_win_profit_v = sum_win_profit_v/num_win_days\n",
        "iter.append('Average loss day profit value')\n",
        "iterV.append(avg_win_profit_v)\n",
        "\n",
        "# win days percent\n",
        "win_days_percent = num_win_days/number_days*100\n",
        "iter.append('Win day %')\n",
        "iterV.append(win_days_percent)\n",
        "\n",
        "# loss days percent\n",
        "loss_days_percent = num_loss_days/number_days*100\n",
        "iter.append('loss day %')\n",
        "iterV.append(loss_days_percent)\n",
        "\n",
        "# max win streak\n",
        "max_win_streak = 0\n",
        "current_streak = 0\n",
        "\n",
        "# Loop through the values in the column\n",
        "for value in cleaned_data['Profit Percentage']:\n",
        "    if pd.notna(value) and value >= 0:\n",
        "        current_streak += 1  # Increase streak for positive values\n",
        "        max_win_streak = max(max_win_streak, current_streak)  # Update max streak if needed\n",
        "    elif value < 0:\n",
        "        current_streak = 0  # Reset streak for negative values\n",
        "    # No action for NaN (empty) values, they won't break the streak\n",
        "iter.append('Max W Streak')\n",
        "iterV.append(max_win_streak)\n",
        "\n",
        "# max loss streak\n",
        "max_loss_streak = 0\n",
        "current_streak = 0\n",
        "\n",
        "# Loop through the values in the column\n",
        "for value in cleaned_data['Profit Percentage']:\n",
        "    if pd.notna(value) and value < 0:\n",
        "        current_streak += 1  # Increase streak for positive values\n",
        "        max_loss_streak = max(max_loss_streak, current_streak)  # Update max streak if needed\n",
        "    elif value >= 0:\n",
        "        current_streak = 0  # Reset streak for negative values\n",
        "    # No action for NaN (empty) values, they won't break the streak\n",
        "iter.append('Max L Streak')\n",
        "iterV.append(max_loss_streak)\n",
        "\n",
        "# max draw down\n",
        "cleaned_data['curval'] = cleaned_data['Profit Value'] + margin\n",
        "cleaned_data['curmax'] = cleaned_data['curval'].cummax()\n",
        "cleaned_data['drawdown'] = (cleaned_data['curval'] - cleaned_data['curmax'])/cleaned_data['curmax']\n",
        "max_drawdown = cleaned_data['drawdown'].min()*-100\n",
        "iter.append('max drawdown')\n",
        "iterV.append(max_drawdown)\n",
        "\n",
        "# max draw down end date\n",
        "max_drawdown_end = cleaned_data['drawdown'].idxmin()\n",
        "end_date = cleaned_data.loc[max_drawdown_end, 'Date']\n",
        "iter.append('MDD End Date')\n",
        "iterV.append(end_date)\n",
        "\n",
        "# max draw down start date\n",
        "max_drawdown_start = cleaned_data.loc[:max_drawdown_end, 'curval'].idxmax()\n",
        "start_date = cleaned_data.loc[max_drawdown_start, 'Date']\n",
        "iter.append('MDD Start Date')\n",
        "iterV.append(start_date)\n",
        "\n",
        "# number of drawdown days\n",
        "start_date = pd.to_datetime(start_date)\n",
        "end_date = pd.to_datetime(end_date)\n",
        "drawdown_days = (end_date - start_date).days\n",
        "iter.append('Number of drawdown days')\n",
        "iterV.append(drawdown_days)\n",
        "\n",
        "# MDD recovery\n",
        "previous_peak = cleaned_data.loc[max_drawdown_start, 'curval']\n",
        "recovery_date_idx = cleaned_data[max_drawdown_end:][cleaned_data['curval'] >= previous_peak].index.min()\n",
        "if pd.notna(recovery_date_idx):\n",
        "    recovery_date = cleaned_data.loc[recovery_date_idx, 'date']\n",
        "    end_date = cleaned_data.loc[max_drawdown_end, 'date']\n",
        "    recovery_days = (recovery_date - end_date).days\n",
        "else:\n",
        "    recovery_days = \"NA\"\n",
        "iter.append('MDD Recovery Days')\n",
        "iterV.append(recovery_days)\n",
        "\n",
        "\n",
        "\"\"\" FOR UPDATING GOOGLE SHEETS \"\"\"\n",
        "# Authenticate user\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get default credentials using google-auth\n",
        "creds, project = default()\n",
        "\n",
        "# Authorize gspread with the credentials\n",
        "client = gspread.authorize(creds)\n",
        "\n",
        "# Open the Google Sheet (replace \"Your Google Sheet Name\" with the actual sheet name)\n",
        "sheet = client.open_by_url(\"https://docs.google.com/spreadsheets/d/1m6kylRZBnOiUDqZe0-_viGpHWpDw08aHeT7mmAUw_Tg/edit?gid=1902718297#gid=1902718297\").sheet1\n",
        "\n",
        "# Handle NaN and infinity values by replacing them with None\n",
        "\n",
        "data = cleaned_data.replace([float('inf'), float('-inf')], None)  # Replace infinities with None\n",
        "data = data.fillna(\"\")  # Replace NaNs with None\n",
        "# data.index = data.index.astype(str)\n",
        "# data = data.reset_index()\n",
        "\n",
        "\n",
        "\n",
        "# Convert the DataFrame to a list of lists\n",
        "data = data.values.tolist()\n",
        "\n",
        "# Clear the existing data in the sheet\n",
        "sheet.clear()\n",
        "\n",
        "# Update the sheet with new data, including column headers\n",
        "sheet.update('A1', [cleaned_data.columns.tolist()] + data)  # A1 will be the header\n",
        "\n",
        "# Get all values of the specific column from Google Sheets\n",
        "col_index = cleaned_data.columns.get_loc(\"Profit Percentage\") + 1  # Adjust for 1-based index in Google Sheets\n",
        "col_values = sheet.col_values(col_index)\n",
        "\n",
        "# Find the next available row in the column\n",
        "next_row = len(col_values) + 2\n",
        "\n",
        "# Update the cell below the last row with the sum\n",
        "\n",
        "for i in range(len(iter)):\n",
        "    sheet.update_cell(next_row+i, col_index, iter[i])\n",
        "    sheet.update_cell(next_row+i, col_index+1, str(iterV[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vKyy_wn1vvNx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}